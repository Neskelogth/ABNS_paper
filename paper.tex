\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Proposition of a New Experiment to Better Understand the Relation Between Typicality and Prototypes}



\author{\IEEEauthorblockN{Samuel Kostadinov}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{University of Trento}\\
Trento, Italy \\
samuel.kostadinov@unitn.it}


\maketitle

\begin{abstract}

	Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum

\end{abstract}

\begin{IEEEkeywords}
Typicality, Prototypes, CNNs, Siamese Network
\end{IEEEkeywords}


\section{Introduction}
	
	The typicality of a concept is a topic that needs a lot of exploration, since it's difficult to evaluate precisely the typicality of an object 
	and also because the people's brains are always a little different from each other. The experiment I would like to propose, has the objective of 
	make sure we can understand better the bond between the perceived typicality of an object that belongs to a category and the prototype of that category.
	This can help the scientists to better understand how knowledge is organized in the brain. This experiment consists in different phases, such as
	data collection, feature extraction, prototype construction and similarity judgment.


\section{Background}

	\subsection{Feature extraction}

		Feature extraction is a topic widely studied. 
		This topic has numerous different applications. 
		One of its goals is to reduce the amount of computational power needed for image processing. 
		There are various techniques to extract meaningful features from images. 
		Some of them are very common and very easy to implement. 
		These are, for example, edge extraction or shape analysis. 
		Other possibilities, more advanced, involve neural networks.\\

			\subsubsection{Classic feature extraction techniques}
				
				One of the first feature extraction methods implemented in image processing is edge detection.				
				In 2019, a paper by Owotogbe presented a review of edge detection techniques. 
				These are usually divided into two groups, gradient-based and Gaussian-based. 
				Some examples are the Sobel operator and the Canny edge detector. 
				Each of the methods has pros and cons. 
				It's the user's job to find the most appropriate for its goal~\cite{1}.\\
				
				After edge detection, other techniques began to spread for feature extraction. 
				In a paper by Kumar and Kumar Bhatia written in 2014 \cite{2}, 
				there are some examples of techniques used to extract features. 
				In the first one, the authors present different types of features and then some techniques to extract them. 
				These are:

				\begin{itemize}
				
					\item \textbf{Diagonal based feature extraction techniques}:
					
						In this procedure, the image is divided into zones formed by small squares of pixels. 
						In this case, there would be 19 diagonal lines. 
						The value of each pixel in these diagonals is summed to obtain a single sub feature. 
						Then we can extract a feature by averaging the sub features. 
						With this method, we can extract a feature for every zone. 
						Then by averaging the column-wise and row-wise features we can increase their number. 
						
					
					\item \textbf{Fourier descriptors}:
					
						The Fourier transform is commonly used for shape analysis. 
						The Fourier transformed coefficient form the Fourier descriptors. 
						These descriptors represent the shape in a frequency domain, with the low frequencies symbolizing the general shape and the high frequencies symbolizing details of the shape. 
						Since the transformation usually generates many parameters, only a subset is considered.
					
					
					\item \textbf{Principal Component Analysis (PCA)}:
					
						This procedure is a mathematical way to convert a set of observations into a set of values of uncorrelated variables. 
						These variables are defined so that the first one has the highest variance, and the components are all orthogonal (independent) from each other. 
						
					\item \textbf{Independent component analysis (ICA)}:
					
						ICA is a statistical technique. 
						It aims to use non-Gaussian random variables to represent multidimensional vectors. 
						The random variables should be as independent as possible. 
					
					\item \textbf{Gabor filter}:
					
						A Gabor is a sinusoid multiplied by a Gaussian, and its response is a convolution operation. 
						This type of filter performs well in both spatial and frequency domains. 
					
					\item \textbf{Chain Code Histogram of Character Contour}:
					
						This method is based on a contour following technique. 
						The contour following uses a chain coding standard proposed by Freeman, that assigns a value to every pixel to identify the next pixel in the border.

					\item \textbf{Finding Intersection/Junction in character}:
					
						Using the same standard proposed by Freeman used in Chain Code Histogram, there is the possibility to count intersections (or junctions) and open ends in a figure.					
						
					\item \textbf{Transition feature}:
					
						This method is based on the transition from background to foreground. 
						There are different techniques that can work both on gray level images or in 4-connected or 8-connected images.
					
				\end{itemize}
			
			
			\noindent These were not the only techniques presented in the paper, but since some were more problem-specific (about handwritten character recognition)
			were excluded from the list. These excluded techniques can still be used, but may have worse results or may require some adaptation. 
			The techniques were: Fractal theory techniques, Shadow features of character, Sector approach for feature extraction, 
			Extraction of distance and angle features, Extraction of occupancy and end-point features, and Zernike moments.\\
			In another paper, written in 2013 by Tian \cite{3}, there are some other techniques cited that may result useful:
			
			\begin{itemize}
			
				\item \textbf{Color features}:
					
					Color is one of the most important features humans can perceive. 
					The features we can extract depend on the color space, but once it's defined there are some possibilities. 
					A few examples are color histograms, color moments and color coherence vectors. 
					One of the simple and meaningful, according to the authors, is color moments. 
					The most common color moments are the mean, the standard deviation and the skewness. 
					
				\item \textbf{Texture features}:
				
					Another very important feature of images is texture. 
					Features involving texture analysis can be extracted from groups of pixels. 
					One of the most common methods is a Gabor filter, which can be used by characterizing the central frequency and the orientation parameters.
			
			\end{itemize}
			
			\subsubsection{Deformable shape analysis}
		
				A more sophisticated technique than the ones listed before is shape analysis. Deformable shape analysis, in particular, can be useful to extract features from an image.\\
				In 2005, Felzenszwalb wrote a paper that focuses on representation and detection of deformable shapes~\cite{4}. For the goal of the experiment proposed here, some deformable shape 
				detection techniques can be useful. \\
				The technique proposed strongly relies on the triangulated polygon representation. This type of representation lets approximate every 2D shape without holes using a representation based on triangles. 
				%The resulting approximation is as precise as the user decides, meaning that it's very flexible for the representation of shapes. 
				%In particular, in the paper the triangulation was done with a technique called constrained Delaunay triangulation (CDT), because of its tight relation with the medial axis transform. 
				The author also make use of the properties of chordal graphs and k-trees.\\
				The technique described in this paper falls under the category of deformable template matching. One of the components of the method is the energy function, a function that associates a cost with every 
				possible transformation. The objective is to find the transformation with the lowest possible cost. This component is very flexible, since depending on the formulation of the energy function the costs 
				can be tuned even for individual triangles. Moreover, it is also possible to integrate learning techniques to learn deformation parameters.
				Since the possible non-rigid transformations of a template are numerous, this kind of techniques usually requires an initialization near to the correct solution, 
				although this is not required for the algorithm presented in this paper. 
				For the implementation of the algorithm itself, a techniques called non-serial dynamic programming was the key factor. In fact, using the order of perfect elimination (property of the every triangulated 
				simple polygon), the algorithm computes the optimal position of every vertex with respect to the other vertices. Once the algorithm solves for the last vertex, it can 
				update all the others as in typical dynamic programming and obtain the optimal location for every vertex.\\
				The learning of the parameters can be seen as looking for the location with the highest. The matching problem is then fed to a statistical framework that computes the configuration minimizing the 
				energy cost, which corresponds to the best template match.
				
			
			\subsubsection{Machine learning based feature extraction techniques} \label{mlfe}
				
				After the classic techniques, machine learning began to be involved in feature extraction. In particular there are some papers that explain the results achieved with machine learning.\\
				In 2019, Halimi et al. wrote a paper about a feature extraction technique that they implemented using unsupervised deep learning~\cite{5}. Although this paper examines the techniques for 
				3D shapes, it's very likely that it can be adapted to extract 2D features from images.\\
				The background of this experiments is formed of some concepts that are not immediate. One of the concepts used as background is the minimum distortion correspondence. The other are, for example, 
				descriptor learning, functional maps and deep functional maps. These concepts serve as a base for all the work done later. \\
				In particular, the authors focus on unsupervised deep functional maps. The authors state that the main contribution of their paper is that they introduce a technique with which they can 
				avoid annotating a lot of data before training a model. After reducing the number of vertices to a number between 5000 and 7000, the authors used a deep functional map network. The inputs are 
				fed to two residual neural networks, which compute a dense descriptor fields. These descriptors, are the inputs to the functional map layer. Lastly, as post processing, a point-wise map recovery was 
				applied and, after that, the shape was upscaled to the original resolution. 
				
				\noindent In 2019, there is another paper, written by Varshni et al. that proposes another method~\cite{6}. This paper has the objective to classify images suing CNNs to understand if a patient 
				has pneumonia or not. After the preprocessing, in which the authors just resized the images, there is the feature extraction process. For this step, the authors used different pretrained models.
				In this paper's results it's reported that the best model architecture is DenseNet-169. This kind of neural network, called Dense Nets, overcomes the problem of gradient vanishing. The model used in 
				particular for the work of this paper had 4 dense blocks and 3 transition layers. Then there is a classification layer. 
				
				\noindent In 2019, Guamei et al. wrote a paper about hybrid feature extraction methods to classify brain tumors. The process described in the paper is divided into three points. The 
				first one is preprocessing, the second is feature extraction and the third is brain tumor classification. The first step is just to rescale the values to the range $[0,\ 1]$. The feature extractor 
				used is the GIST descriptor. The GIST is then combined with a Gabor filter and produces a total of $mxnx4x4$ GIST feature vectors. There is a variant of the GIST descriptor, the 
				PCA-NGIST is a PCA-based normalized GIST feature extraction method. The NGIST is a normalized version of the GIST descriptor, that uses the L2 norm to make the GIST invariant to illumination and shadowing. 
				After this step, the only thing left is the classification of the tumor, which was performed with a RELM classifier. 
				
			\subsubsection{Hyperspectral images feature extraction}
				
				Although there exists techniques that can extract features from hyperspectral images, this kind of images are created to capture the entire spectrum. This fact means that these images can capture 
				more information then what the human eye can. techniques involve, in the majority of the cases, learning of some form, especially in form of neural networks, in particular convolutional recurrent 
				networks, as seen in a paper by Hu et al.~\cite{8} and in a paper by Rasti et al.~\cite{9}. Probably, it's more meaningful to use normal images than these ones, since, as just 

			\subsubsection{Considerations about feature extraction}
			
				Although there are different methods to extract feature, the ones that require neural networks are not easily understandable by humans. The classic feature extraction techniques have a more 
				human comprehensible result, while the machine learning techniques extract feature in the form of vectors of attributes. Both of these can be used in the following steps, but the procedure requires 
				little adjustments in the steps based on the chosen procedure.
		
		\subsection{Networks}
		
			Neural Networks are a powerful instrument in the hands of computer scientists. There exists a lot of different types of artificial neural networks, each with its own peculiarities. In particular, for this 
			experiment the one needed will be, most likely, just convolutional neural networks and siamese neural networks. 
		
			\subsubsection{Convolutional Neural Network (CNN)}
			
				Convolutional neural networks are not an extremely recent type of neural network, especially if we consider that LeNet-5 was created in 1998. Scientists have dealt with CNNs for some years. Now, as of 2022, 
				CNNs are mostly used to deal with images.\\
				In 2017, Wu wrote a paper that had the purpose to serve as an introduction to CNNs~\cite{10}. These networks use mostly three kind of layers: Dense layers (which are fully connected layers), 
				convolution layers and pooling layers. In the following, there will be a short description of each kind of layer. 
				\begin{itemize}
					
					\item \textbf{Dense layer}:\\
						The dense layer is not peculiar of CNNs, in fact it's found in almost all types of neural networks. It consists of a set of neurons that are connected with every neuron of the previous 
						layer. Dense layers are usually used towards the end of the network.
					
					\item \textbf{Convolutional layer}:\\
						Convolutional layers just compute a convolution operation. This helps reducing the input size and extract meaningful features from the image taken as input. These layers are usually used 
						in the first layers of the neural network.
					
					\item \textbf{Pooling layers}:\\
						Pooling layers downsamples the image to reducing the number of parameters needed. These layers are usually alternated with the convolution layers, so are used at the beginning of a neural network.
						
				\end{itemize}
				
				The applications of deep learning and CNNs are numerous. As shown in \ref{mlfe} there are some CNNs used in image classification with the objective to identify diseases. The typical example is 
				the pneumonia detection. Another possibility is to use CNNs in segmentation or self driving cars, as stated in \cite{11} by Alzubaidi et al.
		
			\subsubsection{Siamese Neural Network}


\section{Data Collection}
\section{Feature Extraction}
\section{Prototype Construction}
\section{Similarity Judgment}


\section{Conclusion}
\section{Possible Further Experiments}

%\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
%bottom of columns. Avoid placing them in the middle of columns. Large 
%figures and tables may span across both columns. Figure captions should be 
%below the figures; table heads should appear above the tables. Insert 
%figures and tables after they are cited in the text. Use the abbreviation 
%``Fig.~\ref{fig}'', even at the beginning of a sentence.

%\begin{table}[htbp]
%\caption{Table Type Styles}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%\cline{2-4} 
%\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%\hline
%copy& More table copy$^{\mathrm{a}}$& &  \\
%\hline
%\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%\end{tabular}
%\label{tab1}
%\end{center}
%\end{table}

%\begin{figure}[htbp]
%\centerline{\includegraphics{fig1.png}}
%\caption{Example of a figure caption.}
%\label{fig}
%\end{figure}



\begin{thebibliography}{00}

\bibitem{1} J. S. Owotogbe,T. S. Ibiyemi, and B. A. Adu, Edge Detection Techniques on Digital Images - A Review, International Journal of Innovative Science and Research Technology, vol. 4, issue 11, Nov. 2019
\bibitem{2} Gaurav Kumar, Pradeep Kumar Bhatia, A Detailed Review of Feature Extraction in Image Processing Systems, Fourth International Conference on Advanced Computing \& Communication Technologies, DOI: https://doi.org/10.1109/ACCT.2014.7, 2014
\bibitem{3} Dong Ping Tian, ``A Review on Image Feature Extraction and Representation Techniques'', International Journal of Multimedia and Ubiquitous Engineering, vol.8, pp. 385-396, July 2013
\bibitem{4} Pedro F. Felzenszwalb, ``Representation and Detection of Deformable Shapes'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 2, pp. 208-220, Feb. 2005, doi: https://doi.org/10.1109/TPAMI.2005.35
\bibitem{5} Halimi, Oshri and Litany, Or and Rodola, Emanuele and Bronstein, Alex M and Kimmel, Ron, Unsupervised learning of dense shape correspondence, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4370-4379, 2019
\bibitem{6} Varshni, Dimpy and Thakral, Kartik and Agarwal, Lucky and Nijhawan, Rahul and Mittal, Ankush, Pneumonia detection using CNN based feature extraction, 2019 IEEE international conference on electrical, computer and communication technologies (ICECCT), pp. 1-7, 2019
\bibitem{7} Gumaei, Abdu and Hassan, Mohammad Mehedi and Hassan, Md Rafiul and Alelaiwi, Abdulhameed and Fortino, Giancarlo, A hybrid feature extraction method with regularized extreme learning machine for brain tumor classification, vol. 7, pp. 36266--36273, 2019
\bibitem{8} Hu, Wen-Shuai and Li, Heng-Chao and Pan, Lei and Li, Wei and Tao, Ran and Du, Qian, Spatial--spectral feature extraction via deep ConvLSTM neural networks for hyperspectral image classification, vol. 58, pp. 4237--4250, 2020
\bibitem{9} Rasti, Behnood and Hong, Danfeng and Hang, Renlong and Ghamisi, Pedram and Kang, Xudong and Chanussot, Jocelyn and Benediktsson, Jon Atli, Feature extraction for hyperspectral imagery: The evolution from shallow to deep: Overview and toolbox, IEEE Geoscience and Remote Sensing Magazine, vol.8, pp.60-88, 2020
\bibitem{10} Jianxin Wu, Introduction to convolutional neural networks, National Key Lab for Novel Software Technology. Nanjing University. China, vol. 5, n. 33, 2017
\bibitem{11} Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamar√≠a, J and Fadhel, Mohammed A and Al-Amidie, Muthana and Farhan, Laith, Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions, Journal of big Data, vol. 8, pp. 1-74, 2021

\end{thebibliography}
\end{document}